{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepAutoRec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viikY8IQdVTY",
        "colab_type": "text"
      },
      "source": [
        "# DeepAutoRec\n",
        "\n",
        "- [Training Deep AutoEncoders for Collaborative Filtering](https://arxiv.org/pdf/1708.01715.pdf)\n",
        "\n",
        "## Experiment\n",
        "\n",
        "- Relation of latent dimension and the size of data\n",
        "    - For small size of data, too large latent dimension makes it overfitting\n",
        "- Dropout rate?\n",
        "- Re-feeding?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8FDcw4rVboe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Callable, Tuple\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('bmh')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Input, Reshape, Dense, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.initializers import TruncatedNormal, RandomNormal\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from tensorflow.keras.utils import get_file\n",
        "import zipfile"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdjqodyYVf9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_size : str) -> pd.DataFrame:\n",
        "    ''' load Movie Lens data '''\n",
        "    if data_size == '100k':\n",
        "        file = get_file('ml-100k.zip', 'http://files.grouplens.org/datasets/movielens/ml-100k.zip')\n",
        "        file_name = 'ml-100k/*'\n",
        "    elif data_size == '1m':\n",
        "        file = get_file('ml-1m.zip', 'http://files.grouplens.org/datasets/movielens/ml-1m.zip')\n",
        "        file_name = 'ml-1m/ratings.dat'\n",
        "    elif data_size == '10m':\n",
        "        file = get_file('ml-10m.zip', 'http://files.grouplens.org/datasets/movielens/ml-10m.zip')\n",
        "        file_name = 'ml-10M100K/ratings.dat'\n",
        "    elif data_size == '20m':\n",
        "        file = get_file('ml-20m.zip', 'http://files.grouplens.org/datasets/movielens/ml-20m.zip')\n",
        "        file_name = 'ml-20m/ratings.csv'\n",
        "    zip_ref = zipfile.ZipFile(file, 'r')\n",
        "    zip_ref.extractall()\n",
        "\n",
        "    col_names = ['userId', 'movieId', 'rating', 'timestamp']\n",
        "    ratings = pd.read_csv(file_name, sep = '|', delimiter = '::', names = col_names, engine = 'python')\n",
        "    print(ratings.shape)\n",
        "    return ratings"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZf8Gzf0VhYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "badc11b1-668b-4aa8-f17a-1a62b7bd2de1"
      },
      "source": [
        "ratings = load_data('1m')\n",
        "ratings.head()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000209, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating  timestamp\n",
              "0       1     1193       5  978300760\n",
              "1       1      661       3  978302109\n",
              "2       1      914       3  978301968\n",
              "3       1     3408       4  978300275\n",
              "4       1     2355       5  978824291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGuhgUB67uKF",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q61otQWA3tFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0e59dad-78e9-46cd-8f4b-9c4fd006c812"
      },
      "source": [
        "idx_user_map = ratings.userId.unique()\n",
        "user_idx_map = {e: i for i, e in enumerate(idx_user_map)}\n",
        "n_user = idx_user_map.shape[0]\n",
        "print(n_user)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EMat5wW3nCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c586b04-6614-4d13-bc8b-0ecedab54aee"
      },
      "source": [
        "idx_item_map = ratings.movieId.unique()\n",
        "item_idx_map = {e: i for i, e in enumerate(idx_item_map)}\n",
        "n_item = idx_item_map.shape[0]\n",
        "print(n_item)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJvVnvKX3uNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Id2idx(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.assign(userId = lambda x: x.userId.map(user_idx_map), \n",
        "                     movieId = lambda x: x.movieId.map(item_idx_map))\n",
        "\n",
        "def idx2Id(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.assign(userId = lambda x: x.userId.apply(lambda x: idx_user_map[x]), \n",
        "                     movieId = lambda x: x.movieId.apply(lambda x: idx_item_map[x]))"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS1B1fPw3whx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b7ebe4e7-8957-459d-daf1-92a424327a09"
      },
      "source": [
        "ratings = Id2idx(ratings)\n",
        "ratings.head(5)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating  timestamp\n",
              "0       0        0       5  978300760\n",
              "1       0        1       3  978302109\n",
              "2       0        2       3  978301968\n",
              "3       0        3       4  978300275\n",
              "4       0        4       5  978824291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PTNottGHBv1p",
        "colab": {}
      },
      "source": [
        "def make_interaction(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.groupby('userId', as_index = False)[['movieId', 'rating']].agg(list)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO0a_3NEFstw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_split(df: pd.DataFrame, test_size = 0.1) -> pd.DataFrame:\n",
        "    timeorder = df.groupby(by = 'userId')['timestamp'].rank(method = 'first', ascending = True)\n",
        "    seen_cnt = df.groupby(by = 'userId', as_index = False)['movieId'].agg('count').rename(columns = {'movieId': 'cnts'})\n",
        "\n",
        "    df = df.merge(seen_cnt, how = 'left', on = 'userId')\n",
        "    df = df.assign(timeorder = timeorder)\n",
        "    df = df.assign(split_type = lambda x: np.where(x.timeorder > x.cnts * test_size, 'train', 'test')) # 시간을 기준으로 train과 valid&test 분할\n",
        "\n",
        "    train = df[df.split_type == 'train']\n",
        "    test = df[df.split_type == 'test']\n",
        "    \n",
        "    valid, test = train_test_split(test, test_size = 0.5, random_state = 7777)\n",
        "    valid, test = map(lambda df: df[df.userId.isin(train.userId.unique()) & df.movieId.isin(train.movieId.unique())], (valid, test)) # train에 속하지 않는 user 및 movie 삭제\n",
        "\n",
        "    train, valid, test = map(lambda df: df.reset_index(drop = True), (train, valid, test))\n",
        "    train, valid, test = map(lambda df: df.drop(columns = ['cnts', 'timeorder', 'split_type']), (train, valid, test))\n",
        "\n",
        "    return train, valid, test"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZLAac2E-Bv1u",
        "colab": {}
      },
      "source": [
        "def make_generator(df: pd.DataFrame, \n",
        "                   n_user: int, \n",
        "                   n_item: int, \n",
        "                   batch_size: int, \n",
        "                   user_based = True, \n",
        "                   implicit = False, \n",
        "                   threshold = 3.5) -> Tuple[np.array, np.array]:\n",
        "\n",
        "    n_row = df.index.size\n",
        "    n_col = n_item if user_based else n_user\n",
        "\n",
        "    Ids = np.arange(n_row)\n",
        "    profiles = df['movieId'] if user_based else df['userId']\n",
        "    ratings = df['rating'] if not implicit else df['rating'].apply(lambda x: [1 if r > threshold else 0 for r in x])\n",
        "\n",
        "    n_batch = int(np.ceil(n_row / batch_size))\n",
        "    while True:\n",
        "        np.random.shuffle(Ids)\n",
        "        \n",
        "        for batch_step in range(n_batch):         \n",
        "            lower = batch_step * batch_size\n",
        "            upper = lower + batch_size\n",
        "            \n",
        "            batch_Id = Ids[lower: upper]\n",
        "            batch = np.zeros(shape = (batch_Id.size, n_col))\n",
        "            for i, idx in enumerate(batch_Id):\n",
        "                batch[i, profiles[idx]] = ratings[idx]\n",
        "            \n",
        "            yield batch, batch"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nNPv41eMBv1v",
        "colab": {}
      },
      "source": [
        "def masked_mse(y_true: np.array, y_pred: np.array, masked_value = 0) -> np.array:\n",
        "    mask_true = K.cast(K.not_equal(y_true, masked_value), dtype = 'float32')\n",
        "    masked_se = K.square(mask_true * (y_true - y_pred))\n",
        "    return K.sum(masked_se, axis = -1) / K.sum(mask_true, axis = -1)\n",
        "\n",
        "def masked_rmse(y_true: np.array, y_pred: np.array, masked_value = 0) -> np.array:\n",
        "    mask_true = K.cast(K.not_equal(y_true, masked_value), dtype = 'float32')\n",
        "    masked_se = K.square(mask_true * (y_true - y_pred))\n",
        "    masked_mse = K.sum(masked_se, axis = -1) / K.sum(mask_true, axis = -1)\n",
        "    return K.sqrt(masked_mse)\n",
        "\n",
        "def masked_rmse_clip(y_true: np.array, y_pred: np.array, masked_value = 0) -> np.array:\n",
        "    mask_true = K.cast(K.not_equal(y_true, masked_value), dtype = 'float32')\n",
        "    y_pred = K.clip(y_pred, 0, 5)\n",
        "    masked_se = K.square(mask_true * (y_true - y_pred))\n",
        "    masked_mse = K.sum(masked_se, axis = -1) / K.sum(mask_true, axis = -1)\n",
        "    return K.sqrt(masked_mse)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcXGIVs7PtdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DeepAutoRec(n_user: int, \n",
        "                n_item: int, \n",
        "                n_layer = 3,\n",
        "                latent_dim = 1024,\n",
        "                activation = 'selu', \n",
        "                optimizer = 'adam', \n",
        "                learning_rate = 0.001,\n",
        "                kernel_initializer = None,\n",
        "                kernel_regularizer = None, \n",
        "                dropout_rate = 0.8) -> Callable:\n",
        "\n",
        "    if not kernel_initializer:\n",
        "        kernel_initializer = RandomNormal(mean = 0.0, stddev = 0.05, seed = None)\n",
        "    if not kernel_regularizer:\n",
        "        kernel_regularizer = l2(0.0001)\n",
        "    \n",
        "    input_shape = n_item\n",
        "\n",
        "    # layers\n",
        "    inputs = x = Input(shape = (input_shape, ), name = 'input')\n",
        "    for i in range(n_layer-1):\n",
        "        x = Dense(latent_dim // 2, activation = activation, kernel_initializer = kernel_initializer, kernel_regularizer = kernel_regularizer, name = f'encoder_{i+1}')(x)\n",
        "    x = Dense(latent_dim, activation = activation, kernel_initializer = kernel_initializer, kernel_regularizer = kernel_regularizer, name = f'encoder_{i+2}')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    for i in range(n_layer-1):\n",
        "        x = Dense(latent_dim // 2, activation = activation, kernel_initializer = kernel_initializer, kernel_regularizer = kernel_regularizer, name = f'decoder_{i+1}')(x)\n",
        "    outputs = Dense(input_shape, activation = activation, kernel_initializer = kernel_initializer, kernel_regularizer = kernel_regularizer, name = f'decoder_{i+2}')(x)\n",
        "    \n",
        "    model = Model(inputs = inputs, outputs = [outputs_1, outputs_2])\n",
        "    model.compile(optimizer = optimizer, loss = [masked_mse, ], metrics = [masked_rmse, ])\n",
        "\n",
        "    # re-feeding\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwvA4hA0Ja34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictor(df: pd.DataFrame, \n",
        "                   model: Callable, \n",
        "                   n_user: int, \n",
        "                   n_item: int, \n",
        "                   batch_size: int, \n",
        "                   user_based = True, \n",
        "                   implicit = False, \n",
        "                   threshold = 3.5) -> float:\n",
        "\n",
        "    n_row = df.index.size\n",
        "    n_col = n_item if user_based else n_user\n",
        "\n",
        "    Ids = np.arange(n_row)\n",
        "    profiles = df['movieId'] if user_based else df['userId']\n",
        "    ratings = df['rating'] if not implicit else df['rating'].apply(lambda x: [1 if r > threshold else 0 for r in x])\n",
        "\n",
        "    res: float = 0.0\n",
        "    N: int = 0\n",
        "    steps = int(np.ceil(n_row / batch_size))\n",
        "    for batch_step in range(steps):\n",
        "        lower = batch_step * batch_size\n",
        "        upper = lower + batch_size\n",
        "\n",
        "        batch_Id = Ids[lower: upper]\n",
        "        y_true = np.zeros(shape = (batch_Id.size, n_col))\n",
        "        for i, idx in enumerate(batch_Id):\n",
        "            y_true[i, profiles[idx]] = ratings[idx]\n",
        "\n",
        "        y_pred = model.predict(y_true, verbose = 0)\n",
        "\n",
        "        res += np.sum(masked_rmse(y_true, y_pred).numpy())\n",
        "        N += batch_Id.size\n",
        "    return res / N"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2quTiHQJlpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_history(hist):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n",
        "\n",
        "    ax1.plot(hist.history['loss'])\n",
        "    ax1.plot(hist.history['val_loss'])\n",
        "    ax1.set_title('Loss', fontsize = 20)\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.legend(['loss', 'val_loss'], loc = 'upper right')\n",
        "\n",
        "    ax2.plot(hist.history['masked_rmse'])\n",
        "    ax2.plot(hist.history['val_masked_rmse'])\n",
        "    ax2.set_title('Metric', fontsize = 20)\n",
        "    ax2.set_ylabel('metric')\n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.legend(['masked_rmse', 'val_masked_rmse'], loc = 'upper right')\n",
        "    plt.show()"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDZc_fV0DmRA",
        "colab_type": "text"
      },
      "source": [
        "## 2. DeepAutoRec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF-Fd3g_J4tX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 500\n",
        "batch_size = 128\n",
        "\n",
        "train, valid, test = data_split(ratings)\n",
        "train, valid, test = map(make_interaction, (train, valid, test))\n",
        "train_gen, valid_gen = map(lambda x: make_generator(x, n_user, n_item, batch_size), (train, valid))\n",
        "\n",
        "steps_per_epoch = train.index.size // batch_size + 1\n",
        "validation_steps = valid.index.size // batch_size + 1"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vXapzZTTBv13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "4379454e-f0cc-4473-e1e7-76651fe6473f"
      },
      "source": [
        "# optimizer = SGD(lr = 0.001, decay = 1e-5, momentum = 0.9, nesterov = True)\n",
        "model = DeepAutoRec(n_user, n_item, n_layer = 2, latent_dim = 512, optimizer = 'adam', dropout_rate = 0.4)\n",
        "model.summary()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 3706)]            0         \n",
            "_________________________________________________________________\n",
            "encoder_1 (Dense)            (None, 256)               948992    \n",
            "_________________________________________________________________\n",
            "encoder_2 (Dense)            (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "decoder_1 (Dense)            (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "decoder_2 (Dense)            (None, 3706)              952442    \n",
            "=================================================================\n",
            "Total params: 2,164,346\n",
            "Trainable params: 2,164,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OBjwW0pQBv16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8cc64a0-9b15-4688-93ed-fcede1b1f70d"
      },
      "source": [
        "%%time\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 50)\n",
        "model_checkpoint = ModelCheckpoint('DeepAutoRec_1M.h5', monitor = 'val_loss', mode = 'min', save_best_only = True)\n",
        "\n",
        "hist = model.fit(x = train_gen, validation_data = valid_gen, epochs = epochs,\n",
        "                 steps_per_epoch = steps_per_epoch, validation_steps = validation_steps, \n",
        "                 verbose = 1)#, callbacks = [early_stopping, model_checkpoint])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "48/48 [==============================] - 2s 43ms/step - loss: 4.6642 - masked_rmse: 1.8659 - val_loss: 2.9014 - val_masked_rmse: 1.4114\n",
            "Epoch 2/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 1.8283 - masked_rmse: 1.1470 - val_loss: 2.6670 - val_masked_rmse: 1.3508\n",
            "Epoch 3/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 1.6028 - masked_rmse: 1.0577 - val_loss: 2.4564 - val_masked_rmse: 1.2889\n",
            "Epoch 4/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 1.4661 - masked_rmse: 1.0039 - val_loss: 2.3717 - val_masked_rmse: 1.2643\n",
            "Epoch 5/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 1.3712 - masked_rmse: 0.9659 - val_loss: 2.2251 - val_masked_rmse: 1.2158\n",
            "Epoch 6/500\n",
            "48/48 [==============================] - 2s 42ms/step - loss: 1.2838 - masked_rmse: 0.9299 - val_loss: 2.1788 - val_masked_rmse: 1.2039\n",
            "Epoch 7/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 1.2124 - masked_rmse: 0.8998 - val_loss: 2.1111 - val_masked_rmse: 1.1878\n",
            "Epoch 8/500\n",
            "48/48 [==============================] - 2s 45ms/step - loss: 1.1488 - masked_rmse: 0.8733 - val_loss: 2.0473 - val_masked_rmse: 1.1703\n",
            "Epoch 9/500\n",
            "48/48 [==============================] - 2s 47ms/step - loss: 1.0987 - masked_rmse: 0.8519 - val_loss: 1.8401 - val_masked_rmse: 1.0965\n",
            "Epoch 10/500\n",
            "48/48 [==============================] - 2s 47ms/step - loss: 1.0443 - masked_rmse: 0.8276 - val_loss: 1.9513 - val_masked_rmse: 1.1400\n",
            "Epoch 11/500\n",
            "48/48 [==============================] - 2s 42ms/step - loss: 0.9965 - masked_rmse: 0.8059 - val_loss: 1.8267 - val_masked_rmse: 1.0978\n",
            "Epoch 12/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.9556 - masked_rmse: 0.7867 - val_loss: 1.8722 - val_masked_rmse: 1.1230\n",
            "Epoch 13/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.9222 - masked_rmse: 0.7713 - val_loss: 1.6943 - val_masked_rmse: 1.0592\n",
            "Epoch 14/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.8869 - masked_rmse: 0.7543 - val_loss: 1.6793 - val_masked_rmse: 1.0556\n",
            "Epoch 15/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.8549 - masked_rmse: 0.7394 - val_loss: 1.6126 - val_masked_rmse: 1.0335\n",
            "Epoch 16/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.8259 - masked_rmse: 0.7248 - val_loss: 1.5835 - val_masked_rmse: 1.0269\n",
            "Epoch 17/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.8010 - masked_rmse: 0.7126 - val_loss: 1.5755 - val_masked_rmse: 1.0275\n",
            "Epoch 18/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.7751 - masked_rmse: 0.6999 - val_loss: 1.5616 - val_masked_rmse: 1.0249\n",
            "Epoch 19/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.7529 - masked_rmse: 0.6890 - val_loss: 1.5639 - val_masked_rmse: 1.0252\n",
            "Epoch 20/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.7330 - masked_rmse: 0.6794 - val_loss: 1.5477 - val_masked_rmse: 1.0236\n",
            "Epoch 21/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.7142 - masked_rmse: 0.6699 - val_loss: 1.4874 - val_masked_rmse: 1.0025\n",
            "Epoch 22/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.6964 - masked_rmse: 0.6608 - val_loss: 1.5041 - val_masked_rmse: 1.0080\n",
            "Epoch 23/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.6803 - masked_rmse: 0.6522 - val_loss: 1.3355 - val_masked_rmse: 0.9502\n",
            "Epoch 24/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.6689 - masked_rmse: 0.6476 - val_loss: 1.3914 - val_masked_rmse: 0.9729\n",
            "Epoch 25/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.6512 - masked_rmse: 0.6377 - val_loss: 1.3733 - val_masked_rmse: 0.9715\n",
            "Epoch 26/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.6346 - masked_rmse: 0.6286 - val_loss: 1.3872 - val_masked_rmse: 0.9772\n",
            "Epoch 27/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.6281 - masked_rmse: 0.6259 - val_loss: 1.3052 - val_masked_rmse: 0.9448\n",
            "Epoch 28/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.6169 - masked_rmse: 0.6208 - val_loss: 1.2863 - val_masked_rmse: 0.9399\n",
            "Epoch 29/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.6016 - masked_rmse: 0.6123 - val_loss: 1.2763 - val_masked_rmse: 0.9342\n",
            "Epoch 30/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5876 - masked_rmse: 0.6034 - val_loss: 1.2355 - val_masked_rmse: 0.9241\n",
            "Epoch 31/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5786 - masked_rmse: 0.5993 - val_loss: 1.2604 - val_masked_rmse: 0.9337\n",
            "Epoch 32/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.5686 - masked_rmse: 0.5936 - val_loss: 1.2441 - val_masked_rmse: 0.9248\n",
            "Epoch 33/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5627 - masked_rmse: 0.5915 - val_loss: 1.2262 - val_masked_rmse: 0.9221\n",
            "Epoch 34/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5595 - masked_rmse: 0.5906 - val_loss: 1.1862 - val_masked_rmse: 0.9099\n",
            "Epoch 35/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5469 - masked_rmse: 0.5828 - val_loss: 1.2099 - val_masked_rmse: 0.9168\n",
            "Epoch 36/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5380 - masked_rmse: 0.5776 - val_loss: 1.1675 - val_masked_rmse: 0.9025\n",
            "Epoch 37/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5314 - masked_rmse: 0.5743 - val_loss: 1.1941 - val_masked_rmse: 0.9108\n",
            "Epoch 38/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5254 - masked_rmse: 0.5711 - val_loss: 1.1228 - val_masked_rmse: 0.8840\n",
            "Epoch 39/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5195 - masked_rmse: 0.5679 - val_loss: 1.1826 - val_masked_rmse: 0.9077\n",
            "Epoch 40/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.5139 - masked_rmse: 0.5642 - val_loss: 1.0898 - val_masked_rmse: 0.8725\n",
            "Epoch 41/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5119 - masked_rmse: 0.5639 - val_loss: 1.1476 - val_masked_rmse: 0.8995\n",
            "Epoch 42/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.5070 - masked_rmse: 0.5610 - val_loss: 1.1373 - val_masked_rmse: 0.8941\n",
            "Epoch 43/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.5007 - masked_rmse: 0.5576 - val_loss: 1.1004 - val_masked_rmse: 0.8779\n",
            "Epoch 44/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4984 - masked_rmse: 0.5571 - val_loss: 1.1008 - val_masked_rmse: 0.8791\n",
            "Epoch 45/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4910 - masked_rmse: 0.5520 - val_loss: 1.1012 - val_masked_rmse: 0.8786\n",
            "Epoch 46/500\n",
            "48/48 [==============================] - 2s 38ms/step - loss: 0.4856 - masked_rmse: 0.5485 - val_loss: 1.0772 - val_masked_rmse: 0.8690\n",
            "Epoch 47/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4846 - masked_rmse: 0.5491 - val_loss: 1.0947 - val_masked_rmse: 0.8781\n",
            "Epoch 48/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4825 - masked_rmse: 0.5477 - val_loss: 1.0392 - val_masked_rmse: 0.8555\n",
            "Epoch 49/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4798 - masked_rmse: 0.5463 - val_loss: 1.0278 - val_masked_rmse: 0.8516\n",
            "Epoch 50/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4756 - masked_rmse: 0.5433 - val_loss: 1.0360 - val_masked_rmse: 0.8527\n",
            "Epoch 51/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4723 - masked_rmse: 0.5417 - val_loss: 1.0481 - val_masked_rmse: 0.8618\n",
            "Epoch 52/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4722 - masked_rmse: 0.5420 - val_loss: 1.0485 - val_masked_rmse: 0.8624\n",
            "Epoch 53/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4695 - masked_rmse: 0.5409 - val_loss: 1.0287 - val_masked_rmse: 0.8524\n",
            "Epoch 54/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4639 - masked_rmse: 0.5360 - val_loss: 1.0266 - val_masked_rmse: 0.8530\n",
            "Epoch 55/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4642 - masked_rmse: 0.5368 - val_loss: 1.0394 - val_masked_rmse: 0.8550\n",
            "Epoch 56/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4579 - masked_rmse: 0.5325 - val_loss: 1.0231 - val_masked_rmse: 0.8547\n",
            "Epoch 57/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4557 - masked_rmse: 0.5312 - val_loss: 1.0411 - val_masked_rmse: 0.8589\n",
            "Epoch 58/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4548 - masked_rmse: 0.5310 - val_loss: 0.9941 - val_masked_rmse: 0.8370\n",
            "Epoch 59/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4513 - masked_rmse: 0.5281 - val_loss: 1.0223 - val_masked_rmse: 0.8516\n",
            "Epoch 60/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4493 - masked_rmse: 0.5275 - val_loss: 1.0386 - val_masked_rmse: 0.8576\n",
            "Epoch 61/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4510 - masked_rmse: 0.5291 - val_loss: 0.9778 - val_masked_rmse: 0.8330\n",
            "Epoch 62/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4498 - masked_rmse: 0.5281 - val_loss: 0.9693 - val_masked_rmse: 0.8288\n",
            "Epoch 63/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4451 - masked_rmse: 0.5246 - val_loss: 0.9801 - val_masked_rmse: 0.8337\n",
            "Epoch 64/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4417 - masked_rmse: 0.5222 - val_loss: 0.9825 - val_masked_rmse: 0.8355\n",
            "Epoch 65/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4395 - masked_rmse: 0.5206 - val_loss: 0.9879 - val_masked_rmse: 0.8399\n",
            "Epoch 66/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4393 - masked_rmse: 0.5214 - val_loss: 0.9978 - val_masked_rmse: 0.8392\n",
            "Epoch 67/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4386 - masked_rmse: 0.5203 - val_loss: 0.9819 - val_masked_rmse: 0.8355\n",
            "Epoch 68/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4399 - masked_rmse: 0.5218 - val_loss: 0.9858 - val_masked_rmse: 0.8360\n",
            "Epoch 69/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.4370 - masked_rmse: 0.5193 - val_loss: 0.9737 - val_masked_rmse: 0.8313\n",
            "Epoch 70/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4361 - masked_rmse: 0.5191 - val_loss: 0.9269 - val_masked_rmse: 0.8106\n",
            "Epoch 71/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4346 - masked_rmse: 0.5183 - val_loss: 0.9454 - val_masked_rmse: 0.8208\n",
            "Epoch 72/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4294 - masked_rmse: 0.5144 - val_loss: 0.9496 - val_masked_rmse: 0.8221\n",
            "Epoch 73/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4289 - masked_rmse: 0.5139 - val_loss: 0.9577 - val_masked_rmse: 0.8242\n",
            "Epoch 74/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4286 - masked_rmse: 0.5136 - val_loss: 0.9468 - val_masked_rmse: 0.8180\n",
            "Epoch 75/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4289 - masked_rmse: 0.5142 - val_loss: 0.9754 - val_masked_rmse: 0.8319\n",
            "Epoch 76/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4309 - masked_rmse: 0.5164 - val_loss: 0.9205 - val_masked_rmse: 0.8089\n",
            "Epoch 77/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4286 - masked_rmse: 0.5142 - val_loss: 0.9291 - val_masked_rmse: 0.8086\n",
            "Epoch 78/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4268 - masked_rmse: 0.5128 - val_loss: 0.9300 - val_masked_rmse: 0.8116\n",
            "Epoch 79/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4292 - masked_rmse: 0.5148 - val_loss: 0.9479 - val_masked_rmse: 0.8211\n",
            "Epoch 80/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4258 - masked_rmse: 0.5124 - val_loss: 0.9263 - val_masked_rmse: 0.8116\n",
            "Epoch 81/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4232 - masked_rmse: 0.5106 - val_loss: 0.9461 - val_masked_rmse: 0.8186\n",
            "Epoch 82/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4228 - masked_rmse: 0.5101 - val_loss: 0.9151 - val_masked_rmse: 0.8046\n",
            "Epoch 83/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4236 - masked_rmse: 0.5110 - val_loss: 0.9265 - val_masked_rmse: 0.8125\n",
            "Epoch 84/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4202 - masked_rmse: 0.5087 - val_loss: 0.9199 - val_masked_rmse: 0.8071\n",
            "Epoch 85/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.4197 - masked_rmse: 0.5078 - val_loss: 0.9214 - val_masked_rmse: 0.8116\n",
            "Epoch 86/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4179 - masked_rmse: 0.5067 - val_loss: 0.9130 - val_masked_rmse: 0.8015\n",
            "Epoch 87/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4184 - masked_rmse: 0.5075 - val_loss: 0.9231 - val_masked_rmse: 0.8099\n",
            "Epoch 88/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4196 - masked_rmse: 0.5084 - val_loss: 0.9112 - val_masked_rmse: 0.8016\n",
            "Epoch 89/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4185 - masked_rmse: 0.5074 - val_loss: 0.9105 - val_masked_rmse: 0.8046\n",
            "Epoch 90/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4156 - masked_rmse: 0.5046 - val_loss: 0.9245 - val_masked_rmse: 0.8111\n",
            "Epoch 91/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4170 - masked_rmse: 0.5064 - val_loss: 0.9243 - val_masked_rmse: 0.8098\n",
            "Epoch 92/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4146 - masked_rmse: 0.5044 - val_loss: 0.9176 - val_masked_rmse: 0.8085\n",
            "Epoch 93/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4134 - masked_rmse: 0.5032 - val_loss: 0.9128 - val_masked_rmse: 0.8044\n",
            "Epoch 94/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4117 - masked_rmse: 0.5021 - val_loss: 0.9110 - val_masked_rmse: 0.8061\n",
            "Epoch 95/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4120 - masked_rmse: 0.5023 - val_loss: 0.9371 - val_masked_rmse: 0.8165\n",
            "Epoch 96/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4140 - masked_rmse: 0.5041 - val_loss: 0.8999 - val_masked_rmse: 0.7991\n",
            "Epoch 97/500\n",
            "48/48 [==============================] - 2s 38ms/step - loss: 0.4115 - masked_rmse: 0.5024 - val_loss: 0.9210 - val_masked_rmse: 0.8075\n",
            "Epoch 98/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4145 - masked_rmse: 0.5049 - val_loss: 0.9035 - val_masked_rmse: 0.8026\n",
            "Epoch 99/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4157 - masked_rmse: 0.5060 - val_loss: 0.8912 - val_masked_rmse: 0.7966\n",
            "Epoch 100/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4117 - masked_rmse: 0.5025 - val_loss: 0.9083 - val_masked_rmse: 0.8007\n",
            "Epoch 101/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4115 - masked_rmse: 0.5022 - val_loss: 0.9074 - val_masked_rmse: 0.8029\n",
            "Epoch 102/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4102 - masked_rmse: 0.5011 - val_loss: 0.9188 - val_masked_rmse: 0.8089\n",
            "Epoch 103/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.4093 - masked_rmse: 0.5007 - val_loss: 0.8896 - val_masked_rmse: 0.7954\n",
            "Epoch 104/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4099 - masked_rmse: 0.5013 - val_loss: 0.8820 - val_masked_rmse: 0.7901\n",
            "Epoch 105/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4094 - masked_rmse: 0.5007 - val_loss: 0.8883 - val_masked_rmse: 0.7946\n",
            "Epoch 106/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4100 - masked_rmse: 0.5013 - val_loss: 0.8957 - val_masked_rmse: 0.7949\n",
            "Epoch 107/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4099 - masked_rmse: 0.5014 - val_loss: 0.8870 - val_masked_rmse: 0.7932\n",
            "Epoch 108/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4089 - masked_rmse: 0.5006 - val_loss: 0.8980 - val_masked_rmse: 0.7990\n",
            "Epoch 109/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4049 - masked_rmse: 0.4974 - val_loss: 0.8889 - val_masked_rmse: 0.7925\n",
            "Epoch 110/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4071 - masked_rmse: 0.4993 - val_loss: 0.8713 - val_masked_rmse: 0.7873\n",
            "Epoch 111/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4069 - masked_rmse: 0.4989 - val_loss: 0.9002 - val_masked_rmse: 0.7992\n",
            "Epoch 112/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4072 - masked_rmse: 0.4991 - val_loss: 0.8833 - val_masked_rmse: 0.7906\n",
            "Epoch 113/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4048 - masked_rmse: 0.4974 - val_loss: 0.8925 - val_masked_rmse: 0.7964\n",
            "Epoch 114/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4051 - masked_rmse: 0.4977 - val_loss: 0.8899 - val_masked_rmse: 0.7950\n",
            "Epoch 115/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4030 - masked_rmse: 0.4958 - val_loss: 0.8911 - val_masked_rmse: 0.7928\n",
            "Epoch 116/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4044 - masked_rmse: 0.4970 - val_loss: 0.8821 - val_masked_rmse: 0.7925\n",
            "Epoch 117/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4058 - masked_rmse: 0.4985 - val_loss: 0.8971 - val_masked_rmse: 0.7981\n",
            "Epoch 118/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4053 - masked_rmse: 0.4976 - val_loss: 0.9021 - val_masked_rmse: 0.8013\n",
            "Epoch 119/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4025 - masked_rmse: 0.4953 - val_loss: 0.8964 - val_masked_rmse: 0.7984\n",
            "Epoch 120/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4036 - masked_rmse: 0.4969 - val_loss: 0.8815 - val_masked_rmse: 0.7902\n",
            "Epoch 121/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4030 - masked_rmse: 0.4961 - val_loss: 0.8973 - val_masked_rmse: 0.7987\n",
            "Epoch 122/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4007 - masked_rmse: 0.4939 - val_loss: 0.8861 - val_masked_rmse: 0.7922\n",
            "Epoch 123/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4043 - masked_rmse: 0.4969 - val_loss: 0.8763 - val_masked_rmse: 0.7870\n",
            "Epoch 124/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4038 - masked_rmse: 0.4963 - val_loss: 0.8734 - val_masked_rmse: 0.7850\n",
            "Epoch 125/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4008 - masked_rmse: 0.4936 - val_loss: 0.8831 - val_masked_rmse: 0.7919\n",
            "Epoch 126/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3987 - masked_rmse: 0.4921 - val_loss: 0.8836 - val_masked_rmse: 0.7934\n",
            "Epoch 127/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4016 - masked_rmse: 0.4948 - val_loss: 0.8863 - val_masked_rmse: 0.7930\n",
            "Epoch 128/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3985 - masked_rmse: 0.4920 - val_loss: 0.8722 - val_masked_rmse: 0.7865\n",
            "Epoch 129/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.4001 - masked_rmse: 0.4934 - val_loss: 0.8773 - val_masked_rmse: 0.7870\n",
            "Epoch 130/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.4010 - masked_rmse: 0.4940 - val_loss: 0.8795 - val_masked_rmse: 0.7879\n",
            "Epoch 131/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3994 - masked_rmse: 0.4929 - val_loss: 0.8739 - val_masked_rmse: 0.7868\n",
            "Epoch 132/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3983 - masked_rmse: 0.4915 - val_loss: 0.8663 - val_masked_rmse: 0.7819\n",
            "Epoch 133/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3972 - masked_rmse: 0.4913 - val_loss: 0.8793 - val_masked_rmse: 0.7888\n",
            "Epoch 134/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3952 - masked_rmse: 0.4896 - val_loss: 0.8701 - val_masked_rmse: 0.7840\n",
            "Epoch 135/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3961 - masked_rmse: 0.4904 - val_loss: 0.8810 - val_masked_rmse: 0.7904\n",
            "Epoch 136/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3953 - masked_rmse: 0.4897 - val_loss: 0.8799 - val_masked_rmse: 0.7916\n",
            "Epoch 137/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3961 - masked_rmse: 0.4906 - val_loss: 0.8819 - val_masked_rmse: 0.7935\n",
            "Epoch 138/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3976 - masked_rmse: 0.4917 - val_loss: 0.8802 - val_masked_rmse: 0.7916\n",
            "Epoch 139/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3961 - masked_rmse: 0.4903 - val_loss: 0.8721 - val_masked_rmse: 0.7851\n",
            "Epoch 140/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3942 - masked_rmse: 0.4886 - val_loss: 0.8948 - val_masked_rmse: 0.7993\n",
            "Epoch 141/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3925 - masked_rmse: 0.4872 - val_loss: 0.8659 - val_masked_rmse: 0.7833\n",
            "Epoch 142/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3946 - masked_rmse: 0.4894 - val_loss: 0.8933 - val_masked_rmse: 0.7981\n",
            "Epoch 143/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3940 - masked_rmse: 0.4885 - val_loss: 0.8657 - val_masked_rmse: 0.7821\n",
            "Epoch 144/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3954 - masked_rmse: 0.4896 - val_loss: 0.8822 - val_masked_rmse: 0.7916\n",
            "Epoch 145/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3929 - masked_rmse: 0.4878 - val_loss: 0.8734 - val_masked_rmse: 0.7859\n",
            "Epoch 146/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3939 - masked_rmse: 0.4884 - val_loss: 0.8866 - val_masked_rmse: 0.7955\n",
            "Epoch 147/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3945 - masked_rmse: 0.4895 - val_loss: 0.8706 - val_masked_rmse: 0.7851\n",
            "Epoch 148/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3921 - masked_rmse: 0.4873 - val_loss: 0.8827 - val_masked_rmse: 0.7918\n",
            "Epoch 149/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3931 - masked_rmse: 0.4872 - val_loss: 0.8796 - val_masked_rmse: 0.7916\n",
            "Epoch 150/500\n",
            "48/48 [==============================] - 2s 43ms/step - loss: 0.3949 - masked_rmse: 0.4897 - val_loss: 0.8848 - val_masked_rmse: 0.7946\n",
            "Epoch 151/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3955 - masked_rmse: 0.4899 - val_loss: 0.8740 - val_masked_rmse: 0.7865\n",
            "Epoch 152/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3937 - masked_rmse: 0.4889 - val_loss: 0.8777 - val_masked_rmse: 0.7885\n",
            "Epoch 153/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3932 - masked_rmse: 0.4880 - val_loss: 0.8771 - val_masked_rmse: 0.7903\n",
            "Epoch 154/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3915 - masked_rmse: 0.4865 - val_loss: 0.8788 - val_masked_rmse: 0.7892\n",
            "Epoch 155/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3919 - masked_rmse: 0.4867 - val_loss: 0.8747 - val_masked_rmse: 0.7853\n",
            "Epoch 156/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3904 - masked_rmse: 0.4858 - val_loss: 0.8782 - val_masked_rmse: 0.7934\n",
            "Epoch 157/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3903 - masked_rmse: 0.4858 - val_loss: 0.8842 - val_masked_rmse: 0.7902\n",
            "Epoch 158/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3963 - masked_rmse: 0.4909 - val_loss: 0.8745 - val_masked_rmse: 0.7889\n",
            "Epoch 159/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3930 - masked_rmse: 0.4881 - val_loss: 0.8740 - val_masked_rmse: 0.7858\n",
            "Epoch 160/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3922 - masked_rmse: 0.4867 - val_loss: 0.8878 - val_masked_rmse: 0.7970\n",
            "Epoch 161/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3907 - masked_rmse: 0.4861 - val_loss: 0.8735 - val_masked_rmse: 0.7862\n",
            "Epoch 162/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3908 - masked_rmse: 0.4862 - val_loss: 0.8849 - val_masked_rmse: 0.7937\n",
            "Epoch 163/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3913 - masked_rmse: 0.4864 - val_loss: 0.8814 - val_masked_rmse: 0.7903\n",
            "Epoch 164/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3892 - masked_rmse: 0.4846 - val_loss: 0.8690 - val_masked_rmse: 0.7850\n",
            "Epoch 165/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3884 - masked_rmse: 0.4839 - val_loss: 0.8728 - val_masked_rmse: 0.7885\n",
            "Epoch 166/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3905 - masked_rmse: 0.4863 - val_loss: 0.8894 - val_masked_rmse: 0.7960\n",
            "Epoch 167/500\n",
            "48/48 [==============================] - 2s 45ms/step - loss: 0.3901 - masked_rmse: 0.4852 - val_loss: 0.8762 - val_masked_rmse: 0.7898\n",
            "Epoch 168/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3912 - masked_rmse: 0.4861 - val_loss: 0.8767 - val_masked_rmse: 0.7882\n",
            "Epoch 169/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3905 - masked_rmse: 0.4855 - val_loss: 0.8718 - val_masked_rmse: 0.7875\n",
            "Epoch 170/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3903 - masked_rmse: 0.4857 - val_loss: 0.8757 - val_masked_rmse: 0.7860\n",
            "Epoch 171/500\n",
            "48/48 [==============================] - 2s 43ms/step - loss: 0.3880 - masked_rmse: 0.4839 - val_loss: 0.8742 - val_masked_rmse: 0.7873\n",
            "Epoch 172/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3871 - masked_rmse: 0.4830 - val_loss: 0.8809 - val_masked_rmse: 0.7935\n",
            "Epoch 173/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3858 - masked_rmse: 0.4819 - val_loss: 0.8744 - val_masked_rmse: 0.7880\n",
            "Epoch 174/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3861 - masked_rmse: 0.4819 - val_loss: 0.8660 - val_masked_rmse: 0.7820\n",
            "Epoch 175/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3891 - masked_rmse: 0.4843 - val_loss: 0.8878 - val_masked_rmse: 0.7966\n",
            "Epoch 176/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3889 - masked_rmse: 0.4845 - val_loss: 0.8791 - val_masked_rmse: 0.7914\n",
            "Epoch 177/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3887 - masked_rmse: 0.4845 - val_loss: 0.8710 - val_masked_rmse: 0.7879\n",
            "Epoch 178/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3889 - masked_rmse: 0.4842 - val_loss: 0.8825 - val_masked_rmse: 0.7946\n",
            "Epoch 179/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3901 - masked_rmse: 0.4852 - val_loss: 0.8733 - val_masked_rmse: 0.7864\n",
            "Epoch 180/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3907 - masked_rmse: 0.4859 - val_loss: 0.8740 - val_masked_rmse: 0.7890\n",
            "Epoch 181/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3861 - masked_rmse: 0.4820 - val_loss: 0.8713 - val_masked_rmse: 0.7868\n",
            "Epoch 182/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3867 - masked_rmse: 0.4820 - val_loss: 0.8705 - val_masked_rmse: 0.7867\n",
            "Epoch 183/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3852 - masked_rmse: 0.4807 - val_loss: 0.8722 - val_masked_rmse: 0.7865\n",
            "Epoch 184/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3864 - masked_rmse: 0.4817 - val_loss: 0.8740 - val_masked_rmse: 0.7873\n",
            "Epoch 185/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3863 - masked_rmse: 0.4817 - val_loss: 0.8752 - val_masked_rmse: 0.7884\n",
            "Epoch 186/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3838 - masked_rmse: 0.4801 - val_loss: 0.8780 - val_masked_rmse: 0.7903\n",
            "Epoch 187/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3861 - masked_rmse: 0.4821 - val_loss: 0.8766 - val_masked_rmse: 0.7900\n",
            "Epoch 188/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3878 - masked_rmse: 0.4838 - val_loss: 0.8744 - val_masked_rmse: 0.7890\n",
            "Epoch 189/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3907 - masked_rmse: 0.4862 - val_loss: 0.8762 - val_masked_rmse: 0.7896\n",
            "Epoch 190/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3895 - masked_rmse: 0.4854 - val_loss: 0.8658 - val_masked_rmse: 0.7836\n",
            "Epoch 191/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3836 - masked_rmse: 0.4798 - val_loss: 0.8648 - val_masked_rmse: 0.7820\n",
            "Epoch 192/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3837 - masked_rmse: 0.4795 - val_loss: 0.8738 - val_masked_rmse: 0.7884\n",
            "Epoch 193/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3823 - masked_rmse: 0.4784 - val_loss: 0.8596 - val_masked_rmse: 0.7808\n",
            "Epoch 194/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3805 - masked_rmse: 0.4770 - val_loss: 0.8690 - val_masked_rmse: 0.7854\n",
            "Epoch 195/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3826 - masked_rmse: 0.4792 - val_loss: 0.8724 - val_masked_rmse: 0.7880\n",
            "Epoch 196/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3849 - masked_rmse: 0.4816 - val_loss: 0.8723 - val_masked_rmse: 0.7879\n",
            "Epoch 197/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3853 - masked_rmse: 0.4818 - val_loss: 0.8766 - val_masked_rmse: 0.7878\n",
            "Epoch 198/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3852 - masked_rmse: 0.4813 - val_loss: 0.8690 - val_masked_rmse: 0.7844\n",
            "Epoch 199/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3859 - masked_rmse: 0.4820 - val_loss: 0.8711 - val_masked_rmse: 0.7846\n",
            "Epoch 200/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3846 - masked_rmse: 0.4806 - val_loss: 0.8620 - val_masked_rmse: 0.7831\n",
            "Epoch 201/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3835 - masked_rmse: 0.4800 - val_loss: 0.8639 - val_masked_rmse: 0.7809\n",
            "Epoch 202/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3837 - masked_rmse: 0.4798 - val_loss: 0.8658 - val_masked_rmse: 0.7835\n",
            "Epoch 203/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3824 - masked_rmse: 0.4786 - val_loss: 0.8854 - val_masked_rmse: 0.7944\n",
            "Epoch 204/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3818 - masked_rmse: 0.4778 - val_loss: 0.8651 - val_masked_rmse: 0.7810\n",
            "Epoch 205/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3828 - masked_rmse: 0.4792 - val_loss: 0.8678 - val_masked_rmse: 0.7847\n",
            "Epoch 206/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3819 - masked_rmse: 0.4784 - val_loss: 0.8601 - val_masked_rmse: 0.7806\n",
            "Epoch 207/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3810 - masked_rmse: 0.4776 - val_loss: 0.8718 - val_masked_rmse: 0.7847\n",
            "Epoch 208/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3877 - masked_rmse: 0.4835 - val_loss: 0.8737 - val_masked_rmse: 0.7883\n",
            "Epoch 209/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3839 - masked_rmse: 0.4804 - val_loss: 0.8633 - val_masked_rmse: 0.7797\n",
            "Epoch 210/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3863 - masked_rmse: 0.4823 - val_loss: 0.8719 - val_masked_rmse: 0.7881\n",
            "Epoch 211/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3836 - masked_rmse: 0.4795 - val_loss: 0.8783 - val_masked_rmse: 0.7902\n",
            "Epoch 212/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3804 - masked_rmse: 0.4767 - val_loss: 0.8594 - val_masked_rmse: 0.7806\n",
            "Epoch 213/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3834 - masked_rmse: 0.4794 - val_loss: 0.8617 - val_masked_rmse: 0.7823\n",
            "Epoch 214/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3818 - masked_rmse: 0.4784 - val_loss: 0.8749 - val_masked_rmse: 0.7856\n",
            "Epoch 215/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3825 - masked_rmse: 0.4795 - val_loss: 0.8704 - val_masked_rmse: 0.7851\n",
            "Epoch 216/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3790 - masked_rmse: 0.4758 - val_loss: 0.8632 - val_masked_rmse: 0.7830\n",
            "Epoch 217/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3794 - masked_rmse: 0.4759 - val_loss: 0.8727 - val_masked_rmse: 0.7863\n",
            "Epoch 218/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3801 - masked_rmse: 0.4771 - val_loss: 0.8693 - val_masked_rmse: 0.7844\n",
            "Epoch 219/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3795 - masked_rmse: 0.4765 - val_loss: 0.8598 - val_masked_rmse: 0.7804\n",
            "Epoch 220/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3796 - masked_rmse: 0.4769 - val_loss: 0.8690 - val_masked_rmse: 0.7858\n",
            "Epoch 221/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3799 - masked_rmse: 0.4772 - val_loss: 0.8676 - val_masked_rmse: 0.7835\n",
            "Epoch 222/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3794 - masked_rmse: 0.4763 - val_loss: 0.8755 - val_masked_rmse: 0.7861\n",
            "Epoch 223/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3774 - masked_rmse: 0.4748 - val_loss: 0.8666 - val_masked_rmse: 0.7840\n",
            "Epoch 224/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3789 - masked_rmse: 0.4757 - val_loss: 0.8660 - val_masked_rmse: 0.7836\n",
            "Epoch 225/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3792 - masked_rmse: 0.4766 - val_loss: 0.8663 - val_masked_rmse: 0.7838\n",
            "Epoch 226/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3800 - masked_rmse: 0.4775 - val_loss: 0.8653 - val_masked_rmse: 0.7829\n",
            "Epoch 227/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3783 - masked_rmse: 0.4753 - val_loss: 0.8708 - val_masked_rmse: 0.7865\n",
            "Epoch 228/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3782 - masked_rmse: 0.4755 - val_loss: 0.8694 - val_masked_rmse: 0.7853\n",
            "Epoch 229/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3805 - masked_rmse: 0.4770 - val_loss: 0.8704 - val_masked_rmse: 0.7864\n",
            "Epoch 230/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3812 - masked_rmse: 0.4783 - val_loss: 0.8705 - val_masked_rmse: 0.7850\n",
            "Epoch 231/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3772 - masked_rmse: 0.4742 - val_loss: 0.8665 - val_masked_rmse: 0.7847\n",
            "Epoch 232/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3786 - masked_rmse: 0.4754 - val_loss: 0.8708 - val_masked_rmse: 0.7855\n",
            "Epoch 233/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3806 - masked_rmse: 0.4771 - val_loss: 0.8627 - val_masked_rmse: 0.7815\n",
            "Epoch 234/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3821 - masked_rmse: 0.4787 - val_loss: 0.8707 - val_masked_rmse: 0.7859\n",
            "Epoch 235/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3816 - masked_rmse: 0.4786 - val_loss: 0.8601 - val_masked_rmse: 0.7804\n",
            "Epoch 236/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3815 - masked_rmse: 0.4787 - val_loss: 0.8757 - val_masked_rmse: 0.7886\n",
            "Epoch 237/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3773 - masked_rmse: 0.4745 - val_loss: 0.8637 - val_masked_rmse: 0.7820\n",
            "Epoch 238/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3781 - masked_rmse: 0.4751 - val_loss: 0.8680 - val_masked_rmse: 0.7836\n",
            "Epoch 239/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3796 - masked_rmse: 0.4763 - val_loss: 0.8656 - val_masked_rmse: 0.7830\n",
            "Epoch 240/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3779 - masked_rmse: 0.4753 - val_loss: 0.8731 - val_masked_rmse: 0.7868\n",
            "Epoch 241/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3764 - masked_rmse: 0.4737 - val_loss: 0.8690 - val_masked_rmse: 0.7875\n",
            "Epoch 242/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3765 - masked_rmse: 0.4737 - val_loss: 0.8638 - val_masked_rmse: 0.7801\n",
            "Epoch 243/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3766 - masked_rmse: 0.4741 - val_loss: 0.8717 - val_masked_rmse: 0.7855\n",
            "Epoch 244/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3767 - masked_rmse: 0.4745 - val_loss: 0.8610 - val_masked_rmse: 0.7787\n",
            "Epoch 245/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3760 - masked_rmse: 0.4738 - val_loss: 0.8620 - val_masked_rmse: 0.7827\n",
            "Epoch 246/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3750 - masked_rmse: 0.4729 - val_loss: 0.8635 - val_masked_rmse: 0.7817\n",
            "Epoch 247/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3742 - masked_rmse: 0.4718 - val_loss: 0.8648 - val_masked_rmse: 0.7828\n",
            "Epoch 248/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3762 - masked_rmse: 0.4738 - val_loss: 0.8651 - val_masked_rmse: 0.7809\n",
            "Epoch 249/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3770 - masked_rmse: 0.4744 - val_loss: 0.8772 - val_masked_rmse: 0.7910\n",
            "Epoch 250/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3766 - masked_rmse: 0.4739 - val_loss: 0.8602 - val_masked_rmse: 0.7807\n",
            "Epoch 251/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3769 - masked_rmse: 0.4740 - val_loss: 0.8708 - val_masked_rmse: 0.7828\n",
            "Epoch 252/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3784 - masked_rmse: 0.4759 - val_loss: 0.8589 - val_masked_rmse: 0.7806\n",
            "Epoch 253/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3780 - masked_rmse: 0.4754 - val_loss: 0.8630 - val_masked_rmse: 0.7819\n",
            "Epoch 254/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3766 - masked_rmse: 0.4743 - val_loss: 0.8709 - val_masked_rmse: 0.7854\n",
            "Epoch 255/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3767 - masked_rmse: 0.4743 - val_loss: 0.8609 - val_masked_rmse: 0.7807\n",
            "Epoch 256/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3782 - masked_rmse: 0.4756 - val_loss: 0.8792 - val_masked_rmse: 0.7907\n",
            "Epoch 257/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3756 - masked_rmse: 0.4730 - val_loss: 0.8636 - val_masked_rmse: 0.7824\n",
            "Epoch 258/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3786 - masked_rmse: 0.4747 - val_loss: 0.8665 - val_masked_rmse: 0.7826\n",
            "Epoch 259/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3759 - masked_rmse: 0.4731 - val_loss: 0.8623 - val_masked_rmse: 0.7810\n",
            "Epoch 260/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3747 - masked_rmse: 0.4722 - val_loss: 0.8668 - val_masked_rmse: 0.7842\n",
            "Epoch 261/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3741 - masked_rmse: 0.4717 - val_loss: 0.8700 - val_masked_rmse: 0.7856\n",
            "Epoch 262/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3751 - masked_rmse: 0.4726 - val_loss: 0.8653 - val_masked_rmse: 0.7834\n",
            "Epoch 263/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3759 - masked_rmse: 0.4738 - val_loss: 0.8671 - val_masked_rmse: 0.7856\n",
            "Epoch 264/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3775 - masked_rmse: 0.4743 - val_loss: 0.8591 - val_masked_rmse: 0.7785\n",
            "Epoch 265/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3773 - masked_rmse: 0.4746 - val_loss: 0.8646 - val_masked_rmse: 0.7832\n",
            "Epoch 266/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3740 - masked_rmse: 0.4714 - val_loss: 0.8642 - val_masked_rmse: 0.7848\n",
            "Epoch 267/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3746 - masked_rmse: 0.4720 - val_loss: 0.8614 - val_masked_rmse: 0.7814\n",
            "Epoch 268/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3746 - masked_rmse: 0.4725 - val_loss: 0.8611 - val_masked_rmse: 0.7820\n",
            "Epoch 269/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3744 - masked_rmse: 0.4721 - val_loss: 0.8675 - val_masked_rmse: 0.7847\n",
            "Epoch 270/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3743 - masked_rmse: 0.4722 - val_loss: 0.8628 - val_masked_rmse: 0.7835\n",
            "Epoch 271/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3736 - masked_rmse: 0.4713 - val_loss: 0.8671 - val_masked_rmse: 0.7848\n",
            "Epoch 272/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3763 - masked_rmse: 0.4739 - val_loss: 0.8610 - val_masked_rmse: 0.7810\n",
            "Epoch 273/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3755 - masked_rmse: 0.4733 - val_loss: 0.8723 - val_masked_rmse: 0.7877\n",
            "Epoch 274/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3748 - masked_rmse: 0.4724 - val_loss: 0.8594 - val_masked_rmse: 0.7805\n",
            "Epoch 275/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3746 - masked_rmse: 0.4724 - val_loss: 0.8631 - val_masked_rmse: 0.7825\n",
            "Epoch 276/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3741 - masked_rmse: 0.4718 - val_loss: 0.8662 - val_masked_rmse: 0.7844\n",
            "Epoch 277/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3743 - masked_rmse: 0.4714 - val_loss: 0.8603 - val_masked_rmse: 0.7799\n",
            "Epoch 278/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3739 - masked_rmse: 0.4717 - val_loss: 0.8691 - val_masked_rmse: 0.7860\n",
            "Epoch 279/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3733 - masked_rmse: 0.4711 - val_loss: 0.8639 - val_masked_rmse: 0.7824\n",
            "Epoch 280/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3725 - masked_rmse: 0.4706 - val_loss: 0.8654 - val_masked_rmse: 0.7849\n",
            "Epoch 281/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3722 - masked_rmse: 0.4706 - val_loss: 0.8565 - val_masked_rmse: 0.7786\n",
            "Epoch 282/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3747 - masked_rmse: 0.4732 - val_loss: 0.8774 - val_masked_rmse: 0.7914\n",
            "Epoch 283/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3765 - masked_rmse: 0.4736 - val_loss: 0.8506 - val_masked_rmse: 0.7776\n",
            "Epoch 284/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3775 - masked_rmse: 0.4748 - val_loss: 0.8755 - val_masked_rmse: 0.7858\n",
            "Epoch 285/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3748 - masked_rmse: 0.4731 - val_loss: 0.8583 - val_masked_rmse: 0.7811\n",
            "Epoch 286/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3716 - masked_rmse: 0.4694 - val_loss: 0.8673 - val_masked_rmse: 0.7845\n",
            "Epoch 287/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3732 - masked_rmse: 0.4715 - val_loss: 0.8636 - val_masked_rmse: 0.7826\n",
            "Epoch 288/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3729 - masked_rmse: 0.4709 - val_loss: 0.8665 - val_masked_rmse: 0.7840\n",
            "Epoch 289/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3717 - masked_rmse: 0.4697 - val_loss: 0.8638 - val_masked_rmse: 0.7845\n",
            "Epoch 290/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3708 - masked_rmse: 0.4688 - val_loss: 0.8500 - val_masked_rmse: 0.7756\n",
            "Epoch 291/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3723 - masked_rmse: 0.4706 - val_loss: 0.8644 - val_masked_rmse: 0.7848\n",
            "Epoch 292/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3737 - masked_rmse: 0.4724 - val_loss: 0.8619 - val_masked_rmse: 0.7812\n",
            "Epoch 293/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3724 - masked_rmse: 0.4706 - val_loss: 0.8735 - val_masked_rmse: 0.7882\n",
            "Epoch 294/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3722 - masked_rmse: 0.4702 - val_loss: 0.8566 - val_masked_rmse: 0.7800\n",
            "Epoch 295/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3734 - masked_rmse: 0.4714 - val_loss: 0.8643 - val_masked_rmse: 0.7829\n",
            "Epoch 296/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3742 - masked_rmse: 0.4716 - val_loss: 0.8581 - val_masked_rmse: 0.7803\n",
            "Epoch 297/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3736 - masked_rmse: 0.4718 - val_loss: 0.8543 - val_masked_rmse: 0.7781\n",
            "Epoch 298/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3752 - masked_rmse: 0.4734 - val_loss: 0.8684 - val_masked_rmse: 0.7852\n",
            "Epoch 299/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3765 - masked_rmse: 0.4746 - val_loss: 0.8652 - val_masked_rmse: 0.7833\n",
            "Epoch 300/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3718 - masked_rmse: 0.4702 - val_loss: 0.8575 - val_masked_rmse: 0.7791\n",
            "Epoch 301/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3724 - masked_rmse: 0.4704 - val_loss: 0.8772 - val_masked_rmse: 0.7920\n",
            "Epoch 302/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3720 - masked_rmse: 0.4707 - val_loss: 0.8526 - val_masked_rmse: 0.7768\n",
            "Epoch 303/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3710 - masked_rmse: 0.4695 - val_loss: 0.8658 - val_masked_rmse: 0.7827\n",
            "Epoch 304/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3705 - masked_rmse: 0.4686 - val_loss: 0.8681 - val_masked_rmse: 0.7875\n",
            "Epoch 305/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3705 - masked_rmse: 0.4687 - val_loss: 0.8566 - val_masked_rmse: 0.7792\n",
            "Epoch 306/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3721 - masked_rmse: 0.4706 - val_loss: 0.8630 - val_masked_rmse: 0.7828\n",
            "Epoch 307/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3729 - masked_rmse: 0.4711 - val_loss: 0.8548 - val_masked_rmse: 0.7797\n",
            "Epoch 308/500\n",
            "48/48 [==============================] - 2s 43ms/step - loss: 0.3726 - masked_rmse: 0.4709 - val_loss: 0.8745 - val_masked_rmse: 0.7878\n",
            "Epoch 309/500\n",
            "48/48 [==============================] - 2s 45ms/step - loss: 0.3722 - masked_rmse: 0.4704 - val_loss: 0.8670 - val_masked_rmse: 0.7857\n",
            "Epoch 310/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3710 - masked_rmse: 0.4694 - val_loss: 0.8687 - val_masked_rmse: 0.7854\n",
            "Epoch 311/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3714 - masked_rmse: 0.4697 - val_loss: 0.8614 - val_masked_rmse: 0.7822\n",
            "Epoch 312/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3725 - masked_rmse: 0.4705 - val_loss: 0.8719 - val_masked_rmse: 0.7884\n",
            "Epoch 313/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3720 - masked_rmse: 0.4701 - val_loss: 0.8573 - val_masked_rmse: 0.7798\n",
            "Epoch 314/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3716 - masked_rmse: 0.4694 - val_loss: 0.8653 - val_masked_rmse: 0.7842\n",
            "Epoch 315/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3710 - masked_rmse: 0.4692 - val_loss: 0.8669 - val_masked_rmse: 0.7844\n",
            "Epoch 316/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3707 - masked_rmse: 0.4693 - val_loss: 0.8674 - val_masked_rmse: 0.7857\n",
            "Epoch 317/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3695 - masked_rmse: 0.4682 - val_loss: 0.8634 - val_masked_rmse: 0.7843\n",
            "Epoch 318/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3714 - masked_rmse: 0.4700 - val_loss: 0.8541 - val_masked_rmse: 0.7795\n",
            "Epoch 319/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3701 - masked_rmse: 0.4691 - val_loss: 0.8645 - val_masked_rmse: 0.7833\n",
            "Epoch 320/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3709 - masked_rmse: 0.4694 - val_loss: 0.8626 - val_masked_rmse: 0.7822\n",
            "Epoch 321/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3725 - masked_rmse: 0.4710 - val_loss: 0.8714 - val_masked_rmse: 0.7878\n",
            "Epoch 322/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3722 - masked_rmse: 0.4708 - val_loss: 0.8644 - val_masked_rmse: 0.7843\n",
            "Epoch 323/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3705 - masked_rmse: 0.4688 - val_loss: 0.8562 - val_masked_rmse: 0.7786\n",
            "Epoch 324/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3706 - masked_rmse: 0.4697 - val_loss: 0.8636 - val_masked_rmse: 0.7849\n",
            "Epoch 325/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3697 - masked_rmse: 0.4687 - val_loss: 0.8616 - val_masked_rmse: 0.7810\n",
            "Epoch 326/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3691 - masked_rmse: 0.4681 - val_loss: 0.8591 - val_masked_rmse: 0.7791\n",
            "Epoch 327/500\n",
            "48/48 [==============================] - 2s 43ms/step - loss: 0.3698 - masked_rmse: 0.4685 - val_loss: 0.8603 - val_masked_rmse: 0.7827\n",
            "Epoch 328/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3700 - masked_rmse: 0.4688 - val_loss: 0.8551 - val_masked_rmse: 0.7785\n",
            "Epoch 329/500\n",
            "48/48 [==============================] - 2s 48ms/step - loss: 0.3713 - masked_rmse: 0.4700 - val_loss: 0.8613 - val_masked_rmse: 0.7821\n",
            "Epoch 330/500\n",
            "48/48 [==============================] - 2s 47ms/step - loss: 0.3720 - masked_rmse: 0.4706 - val_loss: 0.8643 - val_masked_rmse: 0.7839\n",
            "Epoch 331/500\n",
            "48/48 [==============================] - 2s 47ms/step - loss: 0.3717 - masked_rmse: 0.4702 - val_loss: 0.8594 - val_masked_rmse: 0.7812\n",
            "Epoch 332/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3706 - masked_rmse: 0.4692 - val_loss: 0.8547 - val_masked_rmse: 0.7786\n",
            "Epoch 333/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3720 - masked_rmse: 0.4702 - val_loss: 0.8543 - val_masked_rmse: 0.7797\n",
            "Epoch 334/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3699 - masked_rmse: 0.4686 - val_loss: 0.8613 - val_masked_rmse: 0.7812\n",
            "Epoch 335/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3709 - masked_rmse: 0.4700 - val_loss: 0.8697 - val_masked_rmse: 0.7864\n",
            "Epoch 336/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3735 - masked_rmse: 0.4720 - val_loss: 0.8614 - val_masked_rmse: 0.7827\n",
            "Epoch 337/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3711 - masked_rmse: 0.4697 - val_loss: 0.8594 - val_masked_rmse: 0.7825\n",
            "Epoch 338/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3713 - masked_rmse: 0.4696 - val_loss: 0.8611 - val_masked_rmse: 0.7826\n",
            "Epoch 339/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3744 - masked_rmse: 0.4721 - val_loss: 0.8693 - val_masked_rmse: 0.7864\n",
            "Epoch 340/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3715 - masked_rmse: 0.4702 - val_loss: 0.8596 - val_masked_rmse: 0.7800\n",
            "Epoch 341/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3696 - masked_rmse: 0.4681 - val_loss: 0.8633 - val_masked_rmse: 0.7841\n",
            "Epoch 342/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3693 - masked_rmse: 0.4680 - val_loss: 0.8644 - val_masked_rmse: 0.7843\n",
            "Epoch 343/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3703 - masked_rmse: 0.4687 - val_loss: 0.8556 - val_masked_rmse: 0.7783\n",
            "Epoch 344/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3698 - masked_rmse: 0.4686 - val_loss: 0.8633 - val_masked_rmse: 0.7825\n",
            "Epoch 345/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3698 - masked_rmse: 0.4682 - val_loss: 0.8636 - val_masked_rmse: 0.7838\n",
            "Epoch 346/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3686 - masked_rmse: 0.4672 - val_loss: 0.8548 - val_masked_rmse: 0.7789\n",
            "Epoch 347/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3679 - masked_rmse: 0.4668 - val_loss: 0.8631 - val_masked_rmse: 0.7833\n",
            "Epoch 348/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3678 - masked_rmse: 0.4666 - val_loss: 0.8660 - val_masked_rmse: 0.7844\n",
            "Epoch 349/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3692 - masked_rmse: 0.4683 - val_loss: 0.8599 - val_masked_rmse: 0.7838\n",
            "Epoch 350/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3689 - masked_rmse: 0.4682 - val_loss: 0.8579 - val_masked_rmse: 0.7825\n",
            "Epoch 351/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3676 - masked_rmse: 0.4666 - val_loss: 0.8723 - val_masked_rmse: 0.7853\n",
            "Epoch 352/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3690 - masked_rmse: 0.4680 - val_loss: 0.8611 - val_masked_rmse: 0.7838\n",
            "Epoch 353/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3684 - masked_rmse: 0.4674 - val_loss: 0.8648 - val_masked_rmse: 0.7854\n",
            "Epoch 354/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3699 - masked_rmse: 0.4685 - val_loss: 0.8606 - val_masked_rmse: 0.7816\n",
            "Epoch 355/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3688 - masked_rmse: 0.4681 - val_loss: 0.8742 - val_masked_rmse: 0.7901\n",
            "Epoch 356/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3712 - masked_rmse: 0.4694 - val_loss: 0.8620 - val_masked_rmse: 0.7846\n",
            "Epoch 357/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3707 - masked_rmse: 0.4693 - val_loss: 0.8685 - val_masked_rmse: 0.7845\n",
            "Epoch 358/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3717 - masked_rmse: 0.4700 - val_loss: 0.8533 - val_masked_rmse: 0.7783\n",
            "Epoch 359/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3704 - masked_rmse: 0.4689 - val_loss: 0.8681 - val_masked_rmse: 0.7858\n",
            "Epoch 360/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3714 - masked_rmse: 0.4693 - val_loss: 0.8676 - val_masked_rmse: 0.7853\n",
            "Epoch 361/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3706 - masked_rmse: 0.4692 - val_loss: 0.8567 - val_masked_rmse: 0.7788\n",
            "Epoch 362/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3740 - masked_rmse: 0.4730 - val_loss: 0.8685 - val_masked_rmse: 0.7856\n",
            "Epoch 363/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3704 - masked_rmse: 0.4690 - val_loss: 0.8635 - val_masked_rmse: 0.7834\n",
            "Epoch 364/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3691 - masked_rmse: 0.4678 - val_loss: 0.8638 - val_masked_rmse: 0.7839\n",
            "Epoch 365/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3666 - masked_rmse: 0.4652 - val_loss: 0.8629 - val_masked_rmse: 0.7834\n",
            "Epoch 366/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3698 - masked_rmse: 0.4685 - val_loss: 0.8525 - val_masked_rmse: 0.7777\n",
            "Epoch 367/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3696 - masked_rmse: 0.4682 - val_loss: 0.8675 - val_masked_rmse: 0.7862\n",
            "Epoch 368/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3705 - masked_rmse: 0.4685 - val_loss: 0.8566 - val_masked_rmse: 0.7800\n",
            "Epoch 369/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3685 - masked_rmse: 0.4676 - val_loss: 0.8651 - val_masked_rmse: 0.7859\n",
            "Epoch 370/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3662 - masked_rmse: 0.4654 - val_loss: 0.8669 - val_masked_rmse: 0.7849\n",
            "Epoch 371/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3655 - masked_rmse: 0.4645 - val_loss: 0.8586 - val_masked_rmse: 0.7803\n",
            "Epoch 372/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3700 - masked_rmse: 0.4691 - val_loss: 0.8675 - val_masked_rmse: 0.7838\n",
            "Epoch 373/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3691 - masked_rmse: 0.4679 - val_loss: 0.8655 - val_masked_rmse: 0.7841\n",
            "Epoch 374/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3654 - masked_rmse: 0.4646 - val_loss: 0.8684 - val_masked_rmse: 0.7884\n",
            "Epoch 375/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3671 - masked_rmse: 0.4661 - val_loss: 0.8601 - val_masked_rmse: 0.7800\n",
            "Epoch 376/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3689 - masked_rmse: 0.4679 - val_loss: 0.8657 - val_masked_rmse: 0.7844\n",
            "Epoch 377/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3683 - masked_rmse: 0.4673 - val_loss: 0.8629 - val_masked_rmse: 0.7826\n",
            "Epoch 378/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3690 - masked_rmse: 0.4679 - val_loss: 0.8584 - val_masked_rmse: 0.7805\n",
            "Epoch 379/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3686 - masked_rmse: 0.4675 - val_loss: 0.8662 - val_masked_rmse: 0.7854\n",
            "Epoch 380/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3679 - masked_rmse: 0.4672 - val_loss: 0.8629 - val_masked_rmse: 0.7817\n",
            "Epoch 381/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3668 - masked_rmse: 0.4662 - val_loss: 0.8663 - val_masked_rmse: 0.7847\n",
            "Epoch 382/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3673 - masked_rmse: 0.4669 - val_loss: 0.8605 - val_masked_rmse: 0.7829\n",
            "Epoch 383/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3662 - masked_rmse: 0.4661 - val_loss: 0.8616 - val_masked_rmse: 0.7811\n",
            "Epoch 384/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3653 - masked_rmse: 0.4642 - val_loss: 0.8650 - val_masked_rmse: 0.7851\n",
            "Epoch 385/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3654 - masked_rmse: 0.4646 - val_loss: 0.8565 - val_masked_rmse: 0.7799\n",
            "Epoch 386/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3666 - masked_rmse: 0.4657 - val_loss: 0.8607 - val_masked_rmse: 0.7824\n",
            "Epoch 387/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3668 - masked_rmse: 0.4662 - val_loss: 0.8789 - val_masked_rmse: 0.7907\n",
            "Epoch 388/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3675 - masked_rmse: 0.4671 - val_loss: 0.8613 - val_masked_rmse: 0.7843\n",
            "Epoch 389/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3675 - masked_rmse: 0.4664 - val_loss: 0.8469 - val_masked_rmse: 0.7752\n",
            "Epoch 390/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3693 - masked_rmse: 0.4683 - val_loss: 0.8529 - val_masked_rmse: 0.7797\n",
            "Epoch 391/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3707 - masked_rmse: 0.4698 - val_loss: 0.8723 - val_masked_rmse: 0.7879\n",
            "Epoch 392/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3675 - masked_rmse: 0.4671 - val_loss: 0.8648 - val_masked_rmse: 0.7853\n",
            "Epoch 393/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3663 - masked_rmse: 0.4654 - val_loss: 0.8624 - val_masked_rmse: 0.7839\n",
            "Epoch 394/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3658 - masked_rmse: 0.4653 - val_loss: 0.8666 - val_masked_rmse: 0.7833\n",
            "Epoch 395/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3674 - masked_rmse: 0.4664 - val_loss: 0.8592 - val_masked_rmse: 0.7840\n",
            "Epoch 396/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3670 - masked_rmse: 0.4663 - val_loss: 0.8608 - val_masked_rmse: 0.7838\n",
            "Epoch 397/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3648 - masked_rmse: 0.4643 - val_loss: 0.8612 - val_masked_rmse: 0.7825\n",
            "Epoch 398/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3671 - masked_rmse: 0.4666 - val_loss: 0.8625 - val_masked_rmse: 0.7848\n",
            "Epoch 399/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3682 - masked_rmse: 0.4678 - val_loss: 0.8617 - val_masked_rmse: 0.7820\n",
            "Epoch 400/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3677 - masked_rmse: 0.4667 - val_loss: 0.8621 - val_masked_rmse: 0.7843\n",
            "Epoch 401/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3684 - masked_rmse: 0.4674 - val_loss: 0.8556 - val_masked_rmse: 0.7805\n",
            "Epoch 402/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3669 - masked_rmse: 0.4662 - val_loss: 0.8554 - val_masked_rmse: 0.7802\n",
            "Epoch 403/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3660 - masked_rmse: 0.4655 - val_loss: 0.8590 - val_masked_rmse: 0.7808\n",
            "Epoch 404/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3675 - masked_rmse: 0.4670 - val_loss: 0.8667 - val_masked_rmse: 0.7870\n",
            "Epoch 405/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3653 - masked_rmse: 0.4648 - val_loss: 0.8591 - val_masked_rmse: 0.7824\n",
            "Epoch 406/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3636 - masked_rmse: 0.4630 - val_loss: 0.8672 - val_masked_rmse: 0.7871\n",
            "Epoch 407/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3656 - masked_rmse: 0.4648 - val_loss: 0.8584 - val_masked_rmse: 0.7819\n",
            "Epoch 408/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3683 - masked_rmse: 0.4671 - val_loss: 0.8633 - val_masked_rmse: 0.7835\n",
            "Epoch 409/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3682 - masked_rmse: 0.4675 - val_loss: 0.8667 - val_masked_rmse: 0.7873\n",
            "Epoch 410/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3667 - masked_rmse: 0.4663 - val_loss: 0.8601 - val_masked_rmse: 0.7825\n",
            "Epoch 411/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3667 - masked_rmse: 0.4660 - val_loss: 0.8653 - val_masked_rmse: 0.7847\n",
            "Epoch 412/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3671 - masked_rmse: 0.4667 - val_loss: 0.8717 - val_masked_rmse: 0.7888\n",
            "Epoch 413/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3674 - masked_rmse: 0.4665 - val_loss: 0.8505 - val_masked_rmse: 0.7765\n",
            "Epoch 414/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3663 - masked_rmse: 0.4658 - val_loss: 0.8597 - val_masked_rmse: 0.7814\n",
            "Epoch 415/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3663 - masked_rmse: 0.4661 - val_loss: 0.8624 - val_masked_rmse: 0.7828\n",
            "Epoch 416/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3673 - masked_rmse: 0.4667 - val_loss: 0.8529 - val_masked_rmse: 0.7775\n",
            "Epoch 417/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3693 - masked_rmse: 0.4686 - val_loss: 0.8618 - val_masked_rmse: 0.7845\n",
            "Epoch 418/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3702 - masked_rmse: 0.4696 - val_loss: 0.8703 - val_masked_rmse: 0.7866\n",
            "Epoch 419/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3680 - masked_rmse: 0.4669 - val_loss: 0.8670 - val_masked_rmse: 0.7852\n",
            "Epoch 420/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3673 - masked_rmse: 0.4663 - val_loss: 0.8624 - val_masked_rmse: 0.7851\n",
            "Epoch 421/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3669 - masked_rmse: 0.4661 - val_loss: 0.8619 - val_masked_rmse: 0.7830\n",
            "Epoch 422/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3654 - masked_rmse: 0.4648 - val_loss: 0.8648 - val_masked_rmse: 0.7864\n",
            "Epoch 423/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3653 - masked_rmse: 0.4645 - val_loss: 0.8565 - val_masked_rmse: 0.7793\n",
            "Epoch 424/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3662 - masked_rmse: 0.4650 - val_loss: 0.8651 - val_masked_rmse: 0.7870\n",
            "Epoch 425/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3665 - masked_rmse: 0.4654 - val_loss: 0.8667 - val_masked_rmse: 0.7841\n",
            "Epoch 426/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3661 - masked_rmse: 0.4655 - val_loss: 0.8630 - val_masked_rmse: 0.7860\n",
            "Epoch 427/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3664 - masked_rmse: 0.4659 - val_loss: 0.8574 - val_masked_rmse: 0.7812\n",
            "Epoch 428/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3660 - masked_rmse: 0.4656 - val_loss: 0.8664 - val_masked_rmse: 0.7882\n",
            "Epoch 429/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3669 - masked_rmse: 0.4663 - val_loss: 0.8651 - val_masked_rmse: 0.7820\n",
            "Epoch 430/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3659 - masked_rmse: 0.4653 - val_loss: 0.8560 - val_masked_rmse: 0.7816\n",
            "Epoch 431/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3644 - masked_rmse: 0.4641 - val_loss: 0.8640 - val_masked_rmse: 0.7852\n",
            "Epoch 432/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3648 - masked_rmse: 0.4645 - val_loss: 0.8576 - val_masked_rmse: 0.7830\n",
            "Epoch 433/500\n",
            "48/48 [==============================] - 2s 42ms/step - loss: 0.3645 - masked_rmse: 0.4644 - val_loss: 0.8773 - val_masked_rmse: 0.7917\n",
            "Epoch 434/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3681 - masked_rmse: 0.4672 - val_loss: 0.8577 - val_masked_rmse: 0.7821\n",
            "Epoch 435/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3677 - masked_rmse: 0.4670 - val_loss: 0.8589 - val_masked_rmse: 0.7820\n",
            "Epoch 436/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3661 - masked_rmse: 0.4660 - val_loss: 0.8644 - val_masked_rmse: 0.7855\n",
            "Epoch 437/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3653 - masked_rmse: 0.4648 - val_loss: 0.8656 - val_masked_rmse: 0.7868\n",
            "Epoch 438/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3653 - masked_rmse: 0.4651 - val_loss: 0.8590 - val_masked_rmse: 0.7808\n",
            "Epoch 439/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3659 - masked_rmse: 0.4653 - val_loss: 0.8650 - val_masked_rmse: 0.7861\n",
            "Epoch 440/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3647 - masked_rmse: 0.4643 - val_loss: 0.8584 - val_masked_rmse: 0.7804\n",
            "Epoch 441/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3664 - masked_rmse: 0.4653 - val_loss: 0.8643 - val_masked_rmse: 0.7837\n",
            "Epoch 442/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3664 - masked_rmse: 0.4657 - val_loss: 0.8588 - val_masked_rmse: 0.7819\n",
            "Epoch 443/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3650 - masked_rmse: 0.4646 - val_loss: 0.8650 - val_masked_rmse: 0.7871\n",
            "Epoch 444/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3652 - masked_rmse: 0.4648 - val_loss: 0.8709 - val_masked_rmse: 0.7863\n",
            "Epoch 445/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3665 - masked_rmse: 0.4663 - val_loss: 0.8578 - val_masked_rmse: 0.7815\n",
            "Epoch 446/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3667 - masked_rmse: 0.4662 - val_loss: 0.8604 - val_masked_rmse: 0.7835\n",
            "Epoch 447/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3645 - masked_rmse: 0.4643 - val_loss: 0.8595 - val_masked_rmse: 0.7828\n",
            "Epoch 448/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3672 - masked_rmse: 0.4665 - val_loss: 0.8656 - val_masked_rmse: 0.7853\n",
            "Epoch 449/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3664 - masked_rmse: 0.4660 - val_loss: 0.8605 - val_masked_rmse: 0.7813\n",
            "Epoch 450/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3644 - masked_rmse: 0.4646 - val_loss: 0.8662 - val_masked_rmse: 0.7863\n",
            "Epoch 451/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3631 - masked_rmse: 0.4632 - val_loss: 0.8563 - val_masked_rmse: 0.7800\n",
            "Epoch 452/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3626 - masked_rmse: 0.4628 - val_loss: 0.8585 - val_masked_rmse: 0.7817\n",
            "Epoch 453/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3624 - masked_rmse: 0.4619 - val_loss: 0.8666 - val_masked_rmse: 0.7862\n",
            "Epoch 454/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3633 - masked_rmse: 0.4630 - val_loss: 0.8580 - val_masked_rmse: 0.7807\n",
            "Epoch 455/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3646 - masked_rmse: 0.4647 - val_loss: 0.8654 - val_masked_rmse: 0.7858\n",
            "Epoch 456/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3667 - masked_rmse: 0.4665 - val_loss: 0.8607 - val_masked_rmse: 0.7827\n",
            "Epoch 457/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3667 - masked_rmse: 0.4666 - val_loss: 0.8586 - val_masked_rmse: 0.7836\n",
            "Epoch 458/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3674 - masked_rmse: 0.4671 - val_loss: 0.8711 - val_masked_rmse: 0.7895\n",
            "Epoch 459/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3669 - masked_rmse: 0.4668 - val_loss: 0.8566 - val_masked_rmse: 0.7823\n",
            "Epoch 460/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3666 - masked_rmse: 0.4661 - val_loss: 0.8602 - val_masked_rmse: 0.7828\n",
            "Epoch 461/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3651 - masked_rmse: 0.4650 - val_loss: 0.8553 - val_masked_rmse: 0.7797\n",
            "Epoch 462/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3657 - masked_rmse: 0.4655 - val_loss: 0.8696 - val_masked_rmse: 0.7880\n",
            "Epoch 463/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3678 - masked_rmse: 0.4673 - val_loss: 0.8603 - val_masked_rmse: 0.7829\n",
            "Epoch 464/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3672 - masked_rmse: 0.4671 - val_loss: 0.8589 - val_masked_rmse: 0.7829\n",
            "Epoch 465/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3649 - masked_rmse: 0.4651 - val_loss: 0.8662 - val_masked_rmse: 0.7855\n",
            "Epoch 466/500\n",
            "48/48 [==============================] - 2s 43ms/step - loss: 0.3646 - masked_rmse: 0.4640 - val_loss: 0.8592 - val_masked_rmse: 0.7826\n",
            "Epoch 467/500\n",
            "48/48 [==============================] - 2s 47ms/step - loss: 0.3649 - masked_rmse: 0.4644 - val_loss: 0.8699 - val_masked_rmse: 0.7864\n",
            "Epoch 468/500\n",
            "48/48 [==============================] - 2s 45ms/step - loss: 0.3636 - masked_rmse: 0.4632 - val_loss: 0.8641 - val_masked_rmse: 0.7860\n",
            "Epoch 469/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3650 - masked_rmse: 0.4641 - val_loss: 0.8538 - val_masked_rmse: 0.7803\n",
            "Epoch 470/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3632 - masked_rmse: 0.4629 - val_loss: 0.8631 - val_masked_rmse: 0.7850\n",
            "Epoch 471/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3649 - masked_rmse: 0.4638 - val_loss: 0.8610 - val_masked_rmse: 0.7827\n",
            "Epoch 472/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3666 - masked_rmse: 0.4657 - val_loss: 0.8556 - val_masked_rmse: 0.7828\n",
            "Epoch 473/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3649 - masked_rmse: 0.4641 - val_loss: 0.8590 - val_masked_rmse: 0.7813\n",
            "Epoch 474/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3646 - masked_rmse: 0.4642 - val_loss: 0.8709 - val_masked_rmse: 0.7870\n",
            "Epoch 475/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3647 - masked_rmse: 0.4648 - val_loss: 0.8581 - val_masked_rmse: 0.7838\n",
            "Epoch 476/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3660 - masked_rmse: 0.4660 - val_loss: 0.8655 - val_masked_rmse: 0.7843\n",
            "Epoch 477/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3654 - masked_rmse: 0.4652 - val_loss: 0.8629 - val_masked_rmse: 0.7842\n",
            "Epoch 478/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3653 - masked_rmse: 0.4644 - val_loss: 0.8604 - val_masked_rmse: 0.7844\n",
            "Epoch 479/500\n",
            "48/48 [==============================] - 2s 39ms/step - loss: 0.3675 - masked_rmse: 0.4662 - val_loss: 0.8673 - val_masked_rmse: 0.7869\n",
            "Epoch 480/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3669 - masked_rmse: 0.4662 - val_loss: 0.8679 - val_masked_rmse: 0.7879\n",
            "Epoch 481/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3641 - masked_rmse: 0.4638 - val_loss: 0.8677 - val_masked_rmse: 0.7871\n",
            "Epoch 482/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3627 - masked_rmse: 0.4621 - val_loss: 0.8571 - val_masked_rmse: 0.7802\n",
            "Epoch 483/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3632 - masked_rmse: 0.4627 - val_loss: 0.8606 - val_masked_rmse: 0.7816\n",
            "Epoch 484/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3615 - masked_rmse: 0.4610 - val_loss: 0.8659 - val_masked_rmse: 0.7870\n",
            "Epoch 485/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3620 - masked_rmse: 0.4621 - val_loss: 0.8616 - val_masked_rmse: 0.7836\n",
            "Epoch 486/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3654 - masked_rmse: 0.4654 - val_loss: 0.8596 - val_masked_rmse: 0.7828\n",
            "Epoch 487/500\n",
            "48/48 [==============================] - 2s 41ms/step - loss: 0.3657 - masked_rmse: 0.4651 - val_loss: 0.8569 - val_masked_rmse: 0.7795\n",
            "Epoch 488/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3654 - masked_rmse: 0.4654 - val_loss: 0.8682 - val_masked_rmse: 0.7896\n",
            "Epoch 489/500\n",
            "48/48 [==============================] - 2s 47ms/step - loss: 0.3633 - masked_rmse: 0.4632 - val_loss: 0.8607 - val_masked_rmse: 0.7829\n",
            "Epoch 490/500\n",
            "48/48 [==============================] - 2s 47ms/step - loss: 0.3633 - masked_rmse: 0.4634 - val_loss: 0.8611 - val_masked_rmse: 0.7826\n",
            "Epoch 491/500\n",
            "48/48 [==============================] - 2s 46ms/step - loss: 0.3626 - masked_rmse: 0.4625 - val_loss: 0.8610 - val_masked_rmse: 0.7839\n",
            "Epoch 492/500\n",
            "48/48 [==============================] - 2s 44ms/step - loss: 0.3624 - masked_rmse: 0.4627 - val_loss: 0.8647 - val_masked_rmse: 0.7863\n",
            "Epoch 493/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3610 - masked_rmse: 0.4610 - val_loss: 0.8526 - val_masked_rmse: 0.7798\n",
            "Epoch 494/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3631 - masked_rmse: 0.4631 - val_loss: 0.8724 - val_masked_rmse: 0.7910\n",
            "Epoch 495/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3644 - masked_rmse: 0.4645 - val_loss: 0.8599 - val_masked_rmse: 0.7823\n",
            "Epoch 496/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3623 - masked_rmse: 0.4625 - val_loss: 0.8641 - val_masked_rmse: 0.7862\n",
            "Epoch 497/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3615 - masked_rmse: 0.4619 - val_loss: 0.8624 - val_masked_rmse: 0.7834\n",
            "Epoch 498/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3625 - masked_rmse: 0.4630 - val_loss: 0.8584 - val_masked_rmse: 0.7808\n",
            "Epoch 499/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3629 - masked_rmse: 0.4629 - val_loss: 0.8568 - val_masked_rmse: 0.7812\n",
            "Epoch 500/500\n",
            "48/48 [==============================] - 2s 40ms/step - loss: 0.3656 - masked_rmse: 0.4652 - val_loss: 0.8630 - val_masked_rmse: 0.7853\n",
            "CPU times: user 19min 10s, sys: 1min 53s, total: 21min 3s\n",
            "Wall time: 16min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25rNsyqNr1qi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "2c27551d-abde-4d28-f2f0-6114abebf510"
      },
      "source": [
        "show_history(hist)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnzAcyi0O_J6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85fd38fe-f463-4f85-aad4-8b06bd841f57"
      },
      "source": [
        "test_pred = make_predictor(test, model, n_user, n_item, batch_size, user_based = True)\n",
        "print(f'Mean of masked RMSE: {test_pred:.4f}')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of masked RMSE: 0.7873\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}